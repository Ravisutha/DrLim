{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Dr LIM - Dimensionality reduction by Learning Invariant Mapping\n",
    "\n",
    "- This paper (similar to TSNE) proposes an alternative method to achieve dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from image_utilities import plot_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## CNN used in the paper \n",
    "\n",
    "![CNN architecture](./images/cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Torch implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DrlimCNN(\n",
       "  (layer_1): Conv2d(1, 15, kernel_size=(6, 6), stride=(1, 1))\n",
       "  (max_pooling): MaxPool2d(kernel_size=19, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer_3): Conv2d(15, 30, kernel_size=(9, 9), stride=(1, 1))\n",
       "  (output_layer): Linear(in_features=30, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DrlimCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrlimCNN, self).__init__()\n",
    "        \n",
    "        # Layer 1:\n",
    "        # n_input_channel = 1\n",
    "        # n_output_channel = 15\n",
    "        # Kernel Size = 8 for padding = 0, stride = 1\n",
    "        k_size = 6\n",
    "        in_channels = 1\n",
    "        out_channels = 15 \n",
    "        self.layer_1 = nn.Conv2d(in_channels, out_channels, k_size)\n",
    "        \n",
    "        # Layer 2: Subsampling - Maxpooling\n",
    "        # Kernel Size = 19  for padding=0 and stride = 1\n",
    "        k_size = 19\n",
    "        self.max_pooling = nn.MaxPool2d(k_size, stride=1)\n",
    "        \n",
    "        # Layer 3: Conv layer\n",
    "        # n_input_channel = 15\n",
    "        # n_output_channel = 30\n",
    "        # Kernel size = 9\n",
    "        in_channels = 15\n",
    "        out_channels = 30\n",
    "        k_size = 9\n",
    "        self.layer_3 = nn.Conv2d(in_channels, out_channels, k_size)\n",
    "        \n",
    "        # Layer 4: Fully connected\n",
    "        self.output_layer = nn.Linear(30, 2)\n",
    "        \n",
    "        # Output probs\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Relu\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add relu on top of conv layer\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Maxpool \n",
    "        x = self.max_pooling(x)\n",
    "        \n",
    "        # Another conv\n",
    "        x = self.layer_3(x)\n",
    "        \n",
    "        # Get the size except for batch\n",
    "        print(\"Shae\")\n",
    "        print(x.shape)\n",
    "        num_flat_features = reduce(lambda x, y: x * y, x.shape[1:])\n",
    "        print(num_flat_features)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(1, num_flat_features)\n",
    "        \n",
    "        # Fully connected \n",
    "        x = self.output_layer(x)\n",
    "        #x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "net = DrlimCNN()\n",
    "\n",
    "# Enable GPU\n",
    "net.to(device)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### Define contrastive loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(output_1, output_2, \n",
    "            target_1, target_2):\n",
    "    if target_1 == target_2:\n",
    "        y = torch.zeros_like(output_1, requires_grad=True)\n",
    "    else:\n",
    "        y = torch.ones_like(output_1, requires_grad=True)\n",
    "        \n",
    "    distance = torch.norm(output_1 - output_2)\n",
    "    ls = torch.pow(distance, 2)\n",
    "    ld = torch.max(torch.zeros_like(output_1), distance)\n",
    "    ld = torch.pow(ld, 2)\n",
    "    \n",
    "    loss = torch.mean((1 - y) * ls + y * ld)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shae\n",
      "torch.Size([1, 30, 1, 1])\n",
      "30\n",
      "Shae\n",
      "torch.Size([1, 30, 1, 1])\n",
      "30\n",
      "---------\n",
      "True\n",
      "True\n",
      "---------\n",
      "Before:\n",
      "tensor([[-0.1469,  0.0284,  0.2501, -0.0105,  0.0645,  0.0915,  0.0288,  0.0407,\n",
      "         -0.0199, -0.0312, -0.0059,  0.0477, -0.1238,  0.0492,  0.1262,  0.0412,\n",
      "          0.0138,  0.0353, -0.0981, -0.1528, -0.0737,  0.0602, -0.0382,  0.0836,\n",
      "         -0.0363, -0.0408, -0.0181,  0.1253, -0.1095,  0.0084],\n",
      "        [ 0.3018, -0.1719, -0.2395,  0.0209,  0.1684, -0.0542, -0.3459,  0.0203,\n",
      "         -0.0885, -0.1709, -0.0121,  0.0994,  0.0604, -0.1789, -0.4296,  0.1218,\n",
      "         -0.1222, -0.0336,  0.3747,  0.1550,  0.3225, -0.1560, -0.0822, -0.1489,\n",
      "          0.0669,  0.0868,  0.1212, -0.1626,  0.0383,  0.1046]],\n",
      "       device='cuda:0')\n",
      "tensor(0.1024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "After:\n",
      "tensor([[-0.0834,  0.0499,  0.2483,  0.1104,  0.0903,  0.1733, -0.0372,  0.0161,\n",
      "          0.0989, -0.0394,  0.0677,  0.0210, -0.1638,  0.0088,  0.1032, -0.1617,\n",
      "          0.0355,  0.2589, -0.2335, -0.1332, -0.0669,  0.2301, -0.0568,  0.0090,\n",
      "          0.0324, -0.1430, -0.0641,  0.1714, -0.2269,  0.0183],\n",
      "        [ 0.2376, -0.1937, -0.2377, -0.1016,  0.1422, -0.1371, -0.2791,  0.0451,\n",
      "         -0.2089, -0.1625, -0.0865,  0.1264,  0.1008, -0.1380, -0.4063,  0.3273,\n",
      "         -0.1442, -0.2601,  0.5118,  0.1352,  0.3155, -0.3280, -0.0634, -0.0733,\n",
      "         -0.0026,  0.1902,  0.1677, -0.2092,  0.1572,  0.0947]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test Code\n",
    "input_1 = torch.randn(1, 1, 32, 32)\n",
    "input_2 = torch.randn(1, 1, 32, 32)\n",
    "\n",
    "out_1 = net(input_1.cuda())\n",
    "out_2 = net(input_2.cuda())\n",
    "out_1 = out_1.requires_grad_(True)\n",
    "out_2 = out_2.requires_grad_(True)\n",
    "\n",
    "print(\"---------\")\n",
    "print(out_1.requires_grad)\n",
    "print(out_2.requires_grad)\n",
    "print(\"---------\")\n",
    "\n",
    "print(\"Before:\")\n",
    "print(net.output_layer.weight.grad)\n",
    "loss = contrastive_loss(out_1, out_2,\n",
    "                        0, 0)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "print(\"After:\")\n",
    "print(net.output_layer.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize(\n",
    "                                       (0.1307,), (0.3081,))\n",
    "                               ])),\n",
    "    batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./files/', train=False, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize(\n",
    "                                       (0.1307,), (0.3081,))\n",
    "                               ])),\n",
    "    batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    # Choose only either 5 or 10 number \n",
    "    filter_index = ((target == 4) | (target == 9))\n",
    "    data = data[filter_index]\n",
    "    target = target[filter_index]\n",
    "    \n",
    "    if batch_idx == 0:\n",
    "        temp = data.numpy()\n",
    "        temp = temp[0:10, ...]\n",
    "        temp = temp.squeeze()\n",
    "        plot_images(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
