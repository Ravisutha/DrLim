{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Dr LIM - Dimensionality reduction by Learning Invariant Mapping\n",
    "\n",
    "- This paper (similar to TSNE) proposes an alternative method to achieve dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from image_utilities import plot_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## CNN used in the paper \n",
    "\n",
    "![CNN architecture](./images/cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Torch implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DrlimCNN(\n",
       "  (layer_1): Conv2d(1, 15, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (max_pooling): MaxPool2d(kernel_size=15, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer_3): Conv2d(15, 30, kernel_size=(10, 10), stride=(1, 1))\n",
       "  (output_layer): Linear(in_features=30, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DrlimCNN(nn.Module):\n",
    "    def __init__(self, n_lower_dim=2):\n",
    "        super(DrlimCNN, self).__init__()\n",
    "        \n",
    "        # Layer 1:\n",
    "        # n_input_channel = 1\n",
    "        # n_output_channel = 15\n",
    "        # Kernel Size = 5 for padding = 0, stride = 1\n",
    "        k_size = 5\n",
    "        in_channels = 1\n",
    "        out_channels = 15 \n",
    "        self.layer_1 = nn.Conv2d(in_channels, out_channels, k_size)\n",
    "        \n",
    "        # Layer 2: Subsampling - Maxpooling\n",
    "        # Kernel Size = 15  for padding=0 and stride = 1\n",
    "        k_size = 15\n",
    "        self.max_pooling = nn.MaxPool2d(k_size, stride=1)\n",
    "        \n",
    "        # Layer 3: Conv layer\n",
    "        # n_input_channel = 15\n",
    "        # n_output_channel = 30\n",
    "        # Kernel size = 10\n",
    "        in_channels = 15\n",
    "        out_channels = 30\n",
    "        k_size = 10\n",
    "        self.layer_3 = nn.Conv2d(in_channels, out_channels, k_size)\n",
    "        \n",
    "        # Layer 4: Fully connected\n",
    "        self.output_layer = nn.Linear(30, n_lower_dim)\n",
    "        \n",
    "        # Output probs\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Relu\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add relu on top of conv layer\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Maxpool \n",
    "        x = self.max_pooling(x)\n",
    "        \n",
    "        # Another conv\n",
    "        x = self.layer_3(x)\n",
    "        \n",
    "        # Get the size except for batch\n",
    "        num_flat_features = reduce(lambda x, y: x * y, x.shape[1:])\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(1, num_flat_features)\n",
    "        \n",
    "        # Fully connected \n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "net = DrlimCNN()\n",
    "\n",
    "# Enable GPU\n",
    "net.to(device)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### Define contrastive loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(output_1, output_2, \n",
    "            target_1, target_2):\n",
    "    if target_1 == target_2:\n",
    "        y = torch.zeros_like(output_1, requires_grad=True)\n",
    "    else:\n",
    "        y = torch.ones_like(output_1, requires_grad=True)\n",
    "        \n",
    "    distance = torch.norm(output_1 - output_2)\n",
    "    ls = torch.pow(distance, 2)\n",
    "    ld = torch.max(torch.zeros_like(output_1), distance)\n",
    "    ld = torch.pow(ld, 2)\n",
    "    \n",
    "    loss = torch.mean((1 - y) * ls + y * ld)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "True\n",
      "True\n",
      "---------\n",
      "Before:\n",
      "None\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "After:\n",
      "tensor([[ 0.0339,  0.0260, -0.0190, -0.0276,  0.0170,  0.0058, -0.0760, -0.0288,\n",
      "         -0.0230, -0.0130,  0.0396, -0.0040, -0.0219, -0.0394, -0.0045, -0.0224,\n",
      "          0.0871,  0.0383, -0.0896,  0.0014, -0.0022, -0.0303, -0.0251, -0.0626,\n",
      "          0.0040,  0.0215,  0.0654,  0.0364,  0.0791, -0.0229],\n",
      "        [-0.0520, -0.0400,  0.0291,  0.0423, -0.0261, -0.0089,  0.1168,  0.0443,\n",
      "          0.0354,  0.0200, -0.0608,  0.0061,  0.0337,  0.0605,  0.0070,  0.0343,\n",
      "         -0.1338, -0.0588,  0.1376, -0.0021,  0.0034,  0.0466,  0.0385,  0.0962,\n",
      "         -0.0062, -0.0331, -0.1005, -0.0559, -0.1215,  0.0351]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test Code\n",
    "input_1 = torch.randn(1, 1, 28, 28)\n",
    "input_2 = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "out_1 = net(input_1.cuda())\n",
    "out_2 = net(input_2.cuda())\n",
    "out_1 = out_1.requires_grad_(True)\n",
    "out_2 = out_2.requires_grad_(True)\n",
    "\n",
    "print(\"---------\")\n",
    "print(out_1.requires_grad)\n",
    "print(out_2.requires_grad)\n",
    "print(\"---------\")\n",
    "\n",
    "print(\"Before:\")\n",
    "print(net.output_layer.weight.grad)\n",
    "loss = contrastive_loss(out_1, out_2,\n",
    "                        0, 0)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "print(\"After:\")\n",
    "print(net.output_layer.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize(\n",
    "                                       (0.1307,), (0.3081,))\n",
    "                               ])),\n",
    "    batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./files/', train=False, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize(\n",
    "                                       (0.1307,), (0.3081,))\n",
    "                               ])),\n",
    "    batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.017\n",
      "[1,  4000] loss: 0.017\n",
      "[1,  6000] loss: 0.017\n",
      "[1,  8000] loss: 0.017\n",
      "[1, 10000] loss: 0.018\n",
      "[2,  2000] loss: 0.017\n",
      "[2,  4000] loss: 0.017\n",
      "[2,  6000] loss: 0.017\n",
      "[2,  8000] loss: 0.017\n",
      "[2, 10000] loss: 0.018\n",
      "[3,  2000] loss: 0.017\n",
      "[3,  4000] loss: 0.017\n",
      "[3,  6000] loss: 0.017\n",
      "[3,  8000] loss: 0.017\n",
      "[3, 10000] loss: 0.018\n",
      "[4,  2000] loss: 0.017\n",
      "[4,  4000] loss: 0.017\n",
      "[4,  6000] loss: 0.017\n",
      "[4,  8000] loss: 0.017\n",
      "[4, 10000] loss: 0.018\n",
      "[5,  2000] loss: 0.017\n",
      "[5,  4000] loss: 0.017\n",
      "[5,  6000] loss: 0.017\n",
      "[5,  8000] loss: 0.017\n",
      "[5, 10000] loss: 0.018\n",
      "[6,  2000] loss: 0.017\n",
      "[6,  4000] loss: 0.017\n",
      "[6,  6000] loss: 0.017\n",
      "[6,  8000] loss: 0.017\n",
      "[6, 10000] loss: 0.018\n",
      "[7,  2000] loss: 0.017\n",
      "[7,  4000] loss: 0.017\n",
      "[7,  6000] loss: 0.017\n",
      "[7,  8000] loss: 0.017\n",
      "[7, 10000] loss: 0.018\n",
      "[8,  2000] loss: 0.017\n",
      "[8,  4000] loss: 0.017\n",
      "[8,  6000] loss: 0.017\n",
      "[8,  8000] loss: 0.017\n",
      "[8, 10000] loss: 0.018\n",
      "[9,  2000] loss: 0.017\n",
      "[9,  4000] loss: 0.017\n",
      "[9,  6000] loss: 0.017\n",
      "[9,  8000] loss: 0.017\n",
      "[9, 10000] loss: 0.018\n",
      "[10,  2000] loss: 0.017\n",
      "[10,  4000] loss: 0.017\n",
      "[10,  6000] loss: 0.017\n",
      "[10,  8000] loss: 0.017\n",
      "[10, 10000] loss: 0.018\n",
      "[11,  2000] loss: 0.017\n",
      "[11,  4000] loss: 0.017\n",
      "[11,  6000] loss: 0.017\n",
      "[11,  8000] loss: 0.017\n",
      "[11, 10000] loss: 0.018\n",
      "[12,  2000] loss: 0.017\n",
      "[12,  4000] loss: 0.017\n",
      "[12,  6000] loss: 0.017\n",
      "[12,  8000] loss: 0.017\n",
      "[12, 10000] loss: 0.018\n",
      "[13,  2000] loss: 0.017\n",
      "[13,  4000] loss: 0.017\n",
      "[13,  6000] loss: 0.017\n",
      "[13,  8000] loss: 0.017\n",
      "[13, 10000] loss: 0.018\n",
      "[14,  2000] loss: 0.017\n",
      "[14,  4000] loss: 0.017\n",
      "[14,  6000] loss: 0.017\n",
      "[14,  8000] loss: 0.017\n",
      "[14, 10000] loss: 0.018\n",
      "[15,  2000] loss: 0.017\n",
      "[15,  4000] loss: 0.017\n",
      "[15,  6000] loss: 0.017\n",
      "[15,  8000] loss: 0.017\n",
      "[15, 10000] loss: 0.018\n",
      "[16,  2000] loss: 0.017\n",
      "[16,  4000] loss: 0.017\n",
      "[16,  6000] loss: 0.017\n",
      "[16,  8000] loss: 0.017\n",
      "[16, 10000] loss: 0.018\n",
      "[17,  2000] loss: 0.017\n",
      "[17,  4000] loss: 0.017\n",
      "[17,  6000] loss: 0.017\n",
      "[17,  8000] loss: 0.017\n",
      "[17, 10000] loss: 0.018\n",
      "[18,  2000] loss: 0.017\n",
      "[18,  4000] loss: 0.017\n",
      "[18,  6000] loss: 0.017\n",
      "[18,  8000] loss: 0.017\n",
      "[18, 10000] loss: 0.018\n",
      "[19,  2000] loss: 0.017\n",
      "[19,  4000] loss: 0.017\n",
      "[19,  6000] loss: 0.017\n",
      "[19,  8000] loss: 0.017\n",
      "[19, 10000] loss: 0.018\n",
      "[20,  2000] loss: 0.017\n",
      "[20,  4000] loss: 0.017\n",
      "[20,  6000] loss: 0.017\n",
      "[20,  8000] loss: 0.017\n",
      "[20, 10000] loss: 0.018\n",
      "[21,  2000] loss: 0.017\n",
      "[21,  4000] loss: 0.017\n",
      "[21,  6000] loss: 0.017\n",
      "[21,  8000] loss: 0.017\n",
      "[21, 10000] loss: 0.018\n",
      "[22,  2000] loss: 0.017\n",
      "[22,  4000] loss: 0.017\n",
      "[22,  6000] loss: 0.017\n",
      "[22,  8000] loss: 0.017\n",
      "[22, 10000] loss: 0.018\n",
      "[23,  2000] loss: 0.017\n",
      "[23,  4000] loss: 0.017\n",
      "[23,  6000] loss: 0.017\n",
      "[23,  8000] loss: 0.017\n",
      "[23, 10000] loss: 0.018\n",
      "[24,  2000] loss: 0.017\n",
      "[24,  4000] loss: 0.017\n",
      "[24,  6000] loss: 0.017\n",
      "[24,  8000] loss: 0.017\n",
      "[24, 10000] loss: 0.018\n",
      "[25,  2000] loss: 0.017\n",
      "[25,  4000] loss: 0.017\n",
      "[25,  6000] loss: 0.017\n",
      "[25,  8000] loss: 0.017\n",
      "[25, 10000] loss: 0.018\n",
      "[26,  2000] loss: 0.017\n",
      "[26,  4000] loss: 0.017\n",
      "[26,  6000] loss: 0.017\n",
      "[26,  8000] loss: 0.017\n",
      "[26, 10000] loss: 0.018\n",
      "[27,  2000] loss: 0.017\n",
      "[27,  4000] loss: 0.017\n",
      "[27,  6000] loss: 0.017\n",
      "[27,  8000] loss: 0.017\n",
      "[27, 10000] loss: 0.018\n",
      "[28,  2000] loss: 0.017\n",
      "[28,  4000] loss: 0.017\n",
      "[28,  6000] loss: 0.017\n",
      "[28,  8000] loss: 0.017\n",
      "[28, 10000] loss: 0.018\n",
      "[29,  2000] loss: 0.017\n",
      "[29,  4000] loss: 0.017\n",
      "[29,  6000] loss: 0.017\n",
      "[29,  8000] loss: 0.017\n",
      "[29, 10000] loss: 0.018\n",
      "[30,  2000] loss: 0.017\n",
      "[30,  4000] loss: 0.017\n",
      "[30,  6000] loss: 0.017\n",
      "[30,  8000] loss: 0.017\n",
      "[30, 10000] loss: 0.018\n",
      "[31,  2000] loss: 0.017\n",
      "[31,  4000] loss: 0.017\n",
      "[31,  6000] loss: 0.017\n",
      "[31,  8000] loss: 0.017\n",
      "[31, 10000] loss: 0.018\n",
      "[32,  2000] loss: 0.017\n",
      "[32,  4000] loss: 0.017\n",
      "[32,  6000] loss: 0.017\n",
      "[32,  8000] loss: 0.017\n",
      "[32, 10000] loss: 0.018\n",
      "[33,  2000] loss: 0.017\n",
      "[33,  4000] loss: 0.017\n",
      "[33,  6000] loss: 0.017\n",
      "[33,  8000] loss: 0.017\n",
      "[33, 10000] loss: 0.018\n",
      "[34,  2000] loss: 0.017\n",
      "[34,  4000] loss: 0.017\n",
      "[34,  6000] loss: 0.017\n",
      "[34,  8000] loss: 0.017\n",
      "[34, 10000] loss: 0.018\n",
      "[35,  2000] loss: 0.017\n",
      "[35,  4000] loss: 0.017\n",
      "[35,  6000] loss: 0.017\n",
      "[35,  8000] loss: 0.017\n",
      "[35, 10000] loss: 0.018\n",
      "[36,  2000] loss: 0.017\n",
      "[36,  4000] loss: 0.017\n",
      "[36,  6000] loss: 0.017\n",
      "[36,  8000] loss: 0.017\n",
      "[36, 10000] loss: 0.018\n",
      "[37,  2000] loss: 0.017\n",
      "[37,  4000] loss: 0.017\n",
      "[37,  6000] loss: 0.017\n",
      "[37,  8000] loss: 0.017\n",
      "[37, 10000] loss: 0.018\n",
      "[38,  2000] loss: 0.017\n",
      "[38,  4000] loss: 0.017\n",
      "[38,  6000] loss: 0.017\n",
      "[38,  8000] loss: 0.017\n",
      "[38, 10000] loss: 0.018\n",
      "[39,  2000] loss: 0.017\n",
      "[39,  4000] loss: 0.017\n",
      "[39,  6000] loss: 0.017\n",
      "[39,  8000] loss: 0.017\n",
      "[39, 10000] loss: 0.018\n",
      "[40,  2000] loss: 0.017\n",
      "[40,  4000] loss: 0.017\n",
      "[40,  6000] loss: 0.017\n",
      "[40,  8000] loss: 0.017\n",
      "[40, 10000] loss: 0.018\n",
      "[41,  2000] loss: 0.017\n",
      "[41,  4000] loss: 0.017\n",
      "[41,  6000] loss: 0.017\n",
      "[41,  8000] loss: 0.017\n",
      "[41, 10000] loss: 0.018\n",
      "[42,  2000] loss: 0.017\n",
      "[42,  4000] loss: 0.017\n",
      "[42,  6000] loss: 0.017\n",
      "[42,  8000] loss: 0.017\n",
      "[42, 10000] loss: 0.018\n",
      "[43,  2000] loss: 0.017\n",
      "[43,  4000] loss: 0.017\n",
      "[43,  6000] loss: 0.017\n",
      "[43,  8000] loss: 0.017\n",
      "[43, 10000] loss: 0.018\n",
      "[44,  2000] loss: 0.017\n",
      "[44,  4000] loss: 0.017\n",
      "[44,  6000] loss: 0.017\n",
      "[44,  8000] loss: 0.017\n",
      "[44, 10000] loss: 0.018\n",
      "[45,  2000] loss: 0.017\n",
      "[45,  4000] loss: 0.017\n",
      "[45,  6000] loss: 0.017\n",
      "[45,  8000] loss: 0.017\n",
      "[45, 10000] loss: 0.018\n",
      "[46,  2000] loss: 0.017\n",
      "[46,  4000] loss: 0.017\n",
      "[46,  6000] loss: 0.017\n",
      "[46,  8000] loss: 0.017\n",
      "[46, 10000] loss: 0.018\n",
      "[47,  2000] loss: 0.017\n",
      "[47,  4000] loss: 0.017\n",
      "[47,  6000] loss: 0.017\n",
      "[47,  8000] loss: 0.017\n",
      "[47, 10000] loss: 0.018\n",
      "[48,  2000] loss: 0.017\n",
      "[48,  4000] loss: 0.017\n",
      "[48,  6000] loss: 0.017\n",
      "[48,  8000] loss: 0.017\n",
      "[48, 10000] loss: 0.018\n",
      "[49,  2000] loss: 0.017\n",
      "[49,  4000] loss: 0.017\n",
      "[49,  6000] loss: 0.017\n",
      "[49,  8000] loss: 0.017\n",
      "[49, 10000] loss: 0.018\n",
      "[50,  2000] loss: 0.017\n",
      "[50,  4000] loss: 0.017\n",
      "[50,  6000] loss: 0.017\n",
      "[50,  8000] loss: 0.017\n",
      "[50, 10000] loss: 0.018\n",
      "[51,  2000] loss: 0.017\n",
      "[51,  4000] loss: 0.017\n",
      "[51,  6000] loss: 0.017\n",
      "[51,  8000] loss: 0.017\n",
      "[51, 10000] loss: 0.018\n",
      "[52,  2000] loss: 0.017\n",
      "[52,  4000] loss: 0.017\n",
      "[52,  6000] loss: 0.017\n",
      "[52,  8000] loss: 0.017\n",
      "[52, 10000] loss: 0.018\n",
      "[53,  2000] loss: 0.017\n",
      "[53,  4000] loss: 0.017\n",
      "[53,  6000] loss: 0.017\n",
      "[53,  8000] loss: 0.017\n",
      "[53, 10000] loss: 0.018\n",
      "[54,  2000] loss: 0.017\n",
      "[54,  4000] loss: 0.017\n",
      "[54,  6000] loss: 0.017\n",
      "[54,  8000] loss: 0.017\n",
      "[54, 10000] loss: 0.018\n",
      "[55,  2000] loss: 0.017\n",
      "[55,  4000] loss: 0.017\n",
      "[55,  6000] loss: 0.017\n",
      "[55,  8000] loss: 0.017\n",
      "[55, 10000] loss: 0.018\n",
      "[56,  2000] loss: 0.017\n",
      "[56,  4000] loss: 0.017\n",
      "[56,  6000] loss: 0.017\n",
      "[56,  8000] loss: 0.017\n",
      "[56, 10000] loss: 0.018\n",
      "[57,  2000] loss: 0.017\n",
      "[57,  4000] loss: 0.017\n",
      "[57,  6000] loss: 0.017\n",
      "[57,  8000] loss: 0.017\n",
      "[57, 10000] loss: 0.018\n",
      "[58,  2000] loss: 0.017\n",
      "[58,  4000] loss: 0.017\n",
      "[58,  6000] loss: 0.017\n",
      "[58,  8000] loss: 0.017\n",
      "[58, 10000] loss: 0.018\n",
      "[59,  2000] loss: 0.017\n",
      "[59,  4000] loss: 0.017\n",
      "[59,  6000] loss: 0.017\n",
      "[59,  8000] loss: 0.017\n",
      "[59, 10000] loss: 0.018\n",
      "[60,  2000] loss: 0.017\n",
      "[60,  4000] loss: 0.017\n",
      "[60,  6000] loss: 0.017\n",
      "[60,  8000] loss: 0.017\n",
      "[60, 10000] loss: 0.018\n",
      "[61,  2000] loss: 0.017\n",
      "[61,  4000] loss: 0.017\n",
      "[61,  6000] loss: 0.017\n",
      "[61,  8000] loss: 0.017\n",
      "[61, 10000] loss: 0.018\n",
      "[62,  2000] loss: 0.017\n",
      "[62,  4000] loss: 0.017\n",
      "[62,  6000] loss: 0.017\n",
      "[62,  8000] loss: 0.017\n",
      "[62, 10000] loss: 0.018\n",
      "[63,  2000] loss: 0.017\n",
      "[63,  4000] loss: 0.017\n",
      "[63,  6000] loss: 0.017\n",
      "[63,  8000] loss: 0.017\n",
      "[63, 10000] loss: 0.018\n",
      "[64,  2000] loss: 0.017\n",
      "[64,  4000] loss: 0.017\n",
      "[64,  6000] loss: 0.017\n",
      "[64,  8000] loss: 0.017\n",
      "[64, 10000] loss: 0.018\n",
      "[65,  2000] loss: 0.017\n",
      "[65,  4000] loss: 0.017\n",
      "[65,  6000] loss: 0.017\n",
      "[65,  8000] loss: 0.017\n",
      "[65, 10000] loss: 0.018\n",
      "[66,  2000] loss: 0.017\n",
      "[66,  4000] loss: 0.017\n",
      "[66,  6000] loss: 0.017\n",
      "[66,  8000] loss: 0.017\n",
      "[66, 10000] loss: 0.018\n",
      "[67,  2000] loss: 0.017\n",
      "[67,  4000] loss: 0.017\n",
      "[67,  6000] loss: 0.017\n",
      "[67,  8000] loss: 0.017\n",
      "[67, 10000] loss: 0.018\n",
      "[68,  2000] loss: 0.017\n",
      "[68,  4000] loss: 0.017\n",
      "[68,  6000] loss: 0.017\n",
      "[68,  8000] loss: 0.017\n",
      "[68, 10000] loss: 0.018\n",
      "[69,  2000] loss: 0.017\n",
      "[69,  4000] loss: 0.017\n",
      "[69,  6000] loss: 0.017\n",
      "[69,  8000] loss: 0.017\n",
      "[69, 10000] loss: 0.018\n",
      "[70,  2000] loss: 0.017\n",
      "[70,  4000] loss: 0.017\n",
      "[70,  6000] loss: 0.017\n",
      "[70,  8000] loss: 0.017\n",
      "[70, 10000] loss: 0.018\n",
      "[71,  2000] loss: 0.017\n",
      "[71,  4000] loss: 0.017\n",
      "[71,  6000] loss: 0.017\n",
      "[71,  8000] loss: 0.017\n",
      "[71, 10000] loss: 0.018\n",
      "[72,  2000] loss: 0.017\n",
      "[72,  4000] loss: 0.017\n",
      "[72,  6000] loss: 0.017\n",
      "[72,  8000] loss: 0.017\n",
      "[72, 10000] loss: 0.018\n",
      "[73,  2000] loss: 0.017\n",
      "[73,  4000] loss: 0.017\n",
      "[73,  6000] loss: 0.017\n",
      "[73,  8000] loss: 0.017\n",
      "[73, 10000] loss: 0.018\n",
      "[74,  2000] loss: 0.017\n",
      "[74,  4000] loss: 0.017\n",
      "[74,  6000] loss: 0.017\n",
      "[74,  8000] loss: 0.017\n",
      "[74, 10000] loss: 0.018\n",
      "[75,  2000] loss: 0.017\n",
      "[75,  4000] loss: 0.017\n",
      "[75,  6000] loss: 0.017\n",
      "[75,  8000] loss: 0.017\n",
      "[75, 10000] loss: 0.018\n",
      "[76,  2000] loss: 0.017\n",
      "[76,  4000] loss: 0.017\n",
      "[76,  6000] loss: 0.017\n",
      "[76,  8000] loss: 0.017\n",
      "[76, 10000] loss: 0.018\n",
      "[77,  2000] loss: 0.017\n",
      "[77,  4000] loss: 0.017\n",
      "[77,  6000] loss: 0.017\n",
      "[77,  8000] loss: 0.017\n",
      "[77, 10000] loss: 0.018\n",
      "[78,  2000] loss: 0.017\n",
      "[78,  4000] loss: 0.017\n",
      "[78,  6000] loss: 0.017\n",
      "[78,  8000] loss: 0.017\n",
      "[78, 10000] loss: 0.018\n",
      "[79,  2000] loss: 0.017\n",
      "[79,  4000] loss: 0.017\n",
      "[79,  6000] loss: 0.017\n",
      "[79,  8000] loss: 0.017\n",
      "[79, 10000] loss: 0.018\n",
      "[80,  2000] loss: 0.017\n",
      "[80,  4000] loss: 0.017\n",
      "[80,  6000] loss: 0.017\n",
      "[80,  8000] loss: 0.017\n",
      "[80, 10000] loss: 0.018\n",
      "[81,  2000] loss: 0.017\n",
      "[81,  4000] loss: 0.017\n",
      "[81,  6000] loss: 0.017\n",
      "[81,  8000] loss: 0.017\n",
      "[81, 10000] loss: 0.018\n",
      "[82,  2000] loss: 0.017\n",
      "[82,  4000] loss: 0.017\n",
      "[82,  6000] loss: 0.017\n",
      "[82,  8000] loss: 0.017\n",
      "[82, 10000] loss: 0.018\n",
      "[83,  2000] loss: 0.017\n",
      "[83,  4000] loss: 0.017\n",
      "[83,  6000] loss: 0.017\n",
      "[83,  8000] loss: 0.017\n",
      "[83, 10000] loss: 0.018\n",
      "[84,  2000] loss: 0.017\n",
      "[84,  4000] loss: 0.017\n",
      "[84,  6000] loss: 0.017\n",
      "[84,  8000] loss: 0.017\n",
      "[84, 10000] loss: 0.018\n",
      "[85,  2000] loss: 0.017\n",
      "[85,  4000] loss: 0.017\n",
      "[85,  6000] loss: 0.017\n",
      "[85,  8000] loss: 0.017\n",
      "[85, 10000] loss: 0.018\n",
      "[86,  2000] loss: 0.017\n",
      "[86,  4000] loss: 0.017\n",
      "[86,  6000] loss: 0.017\n",
      "[86,  8000] loss: 0.017\n",
      "[86, 10000] loss: 0.018\n",
      "[87,  2000] loss: 0.017\n",
      "[87,  4000] loss: 0.017\n",
      "[87,  6000] loss: 0.017\n",
      "[87,  8000] loss: 0.017\n",
      "[87, 10000] loss: 0.018\n",
      "[88,  2000] loss: 0.017\n",
      "[88,  4000] loss: 0.017\n",
      "[88,  6000] loss: 0.017\n",
      "[88,  8000] loss: 0.017\n",
      "[88, 10000] loss: 0.018\n",
      "[89,  2000] loss: 0.017\n",
      "[89,  4000] loss: 0.017\n",
      "[89,  6000] loss: 0.017\n",
      "[89,  8000] loss: 0.017\n",
      "[89, 10000] loss: 0.018\n",
      "[90,  2000] loss: 0.017\n",
      "[90,  4000] loss: 0.017\n",
      "[90,  6000] loss: 0.017\n",
      "[90,  8000] loss: 0.017\n",
      "[90, 10000] loss: 0.018\n",
      "[91,  2000] loss: 0.017\n",
      "[91,  4000] loss: 0.017\n",
      "[91,  6000] loss: 0.017\n",
      "[91,  8000] loss: 0.017\n",
      "[91, 10000] loss: 0.018\n",
      "[92,  2000] loss: 0.017\n",
      "[92,  4000] loss: 0.017\n",
      "[92,  6000] loss: 0.017\n",
      "[92,  8000] loss: 0.017\n",
      "[92, 10000] loss: 0.018\n",
      "[93,  2000] loss: 0.017\n",
      "[93,  4000] loss: 0.017\n",
      "[93,  6000] loss: 0.017\n",
      "[93,  8000] loss: 0.017\n",
      "[93, 10000] loss: 0.018\n",
      "[94,  2000] loss: 0.017\n",
      "[94,  4000] loss: 0.017\n",
      "[94,  6000] loss: 0.017\n",
      "[94,  8000] loss: 0.017\n",
      "[94, 10000] loss: 0.018\n",
      "[95,  2000] loss: 0.017\n",
      "[95,  4000] loss: 0.017\n",
      "[95,  6000] loss: 0.017\n",
      "[95,  8000] loss: 0.017\n",
      "[95, 10000] loss: 0.018\n",
      "[96,  2000] loss: 0.017\n",
      "[96,  4000] loss: 0.017\n",
      "[96,  6000] loss: 0.017\n",
      "[96,  8000] loss: 0.017\n",
      "[96, 10000] loss: 0.018\n",
      "[97,  2000] loss: 0.017\n",
      "[97,  4000] loss: 0.017\n",
      "[97,  6000] loss: 0.017\n",
      "[97,  8000] loss: 0.017\n",
      "[97, 10000] loss: 0.018\n",
      "[98,  2000] loss: 0.017\n",
      "[98,  4000] loss: 0.017\n",
      "[98,  6000] loss: 0.017\n",
      "[98,  8000] loss: 0.017\n",
      "[98, 10000] loss: 0.018\n",
      "[99,  2000] loss: 0.017\n",
      "[99,  4000] loss: 0.017\n",
      "[99,  6000] loss: 0.017\n",
      "[99,  8000] loss: 0.017\n",
      "[99, 10000] loss: 0.018\n",
      "[100,  2000] loss: 0.017\n",
      "[100,  4000] loss: 0.017\n",
      "[100,  6000] loss: 0.017\n",
      "[100,  8000] loss: 0.017\n",
      "[100, 10000] loss: 0.018\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "all_data = []\n",
    "all_target = []\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    # Choose only either 5 or 10 number \n",
    "    filter_index = ((target == 4) | (target == 9))\n",
    "    all_data += data[filter_index].numpy().tolist()\n",
    "    all_target += target[filter_index].numpy().tolist()\n",
    "\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i in range(len(all_data) - 1): \n",
    "        data_1 = torch.tensor(all_data[i])\n",
    "        data_1 = torch.unsqueeze(data_1, 0)\n",
    "        target_1 = torch.tensor(all_target[i])\n",
    "        \n",
    "        data_2 = torch.tensor(all_data[i + 1])\n",
    "        data_2 = torch.unsqueeze(data_2, 0)\n",
    "        target_2 = torch.tensor(all_target[i + 1])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Enable GPUs\n",
    "        input_1, label_1 = data_1.to(device), target_1.to(device)\n",
    "        input_2, label_2 = data_2.to(device), target_2.to(device)\n",
    "        \n",
    "        out_1 = net(input_1)\n",
    "        out_2 = net(input_2)\n",
    "        out_1 = out_1.requires_grad_(True)\n",
    "        out_2 = out_2.requires_grad_(True)\n",
    "\n",
    "        loss = contrastive_loss(out_1, out_2,\n",
    "                                label_1, label_2)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f\\r' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print(\"\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_4 = []\n",
    "target_4 = []\n",
    "\n",
    "data_11 = []\n",
    "target_11 = []\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    # Choose only either 5 or 10 number \n",
    "    filter_index = (target == 4)\n",
    "    data_4 += data[filter_index].numpy().tolist()\n",
    "    target_4 += target[filter_index].numpy().tolist()\n",
    "    \n",
    "    filter_index = (target == 9)\n",
    "    data_11 += data[filter_index].numpy().tolist()\n",
    "    target_11 += target[filter_index].numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Manifold\n",
    "data_manifold_4 = [] \n",
    "data_manifold_11 = [] \n",
    "\n",
    "for i in range(len(data_4)):\n",
    "    input_4 = torch.tensor(data_4[i]).unsqueeze(0).to(device)\n",
    "    out = net(input_4)\n",
    "    out = out.clone()\n",
    "    out = out.cpu().detach().numpy().squeeze()\n",
    "    data_manifold_4.append(out)\n",
    "    \n",
    "data_manifold_4 = np.array(data_manifold_4)\n",
    "    \n",
    "for i in range(len(data_11)):\n",
    "    input_11 = torch.tensor(data_11[i]).unsqueeze(0).to(device)\n",
    "    out = net(input_11)\n",
    "    out = out.clone()\n",
    "    out = out.cpu().detach().numpy().squeeze()\n",
    "    data_manifold_11.append(out)\n",
    "    \n",
    "data_manifold_11 = np.array(data_manifold_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_11_x = data_manifold_11[:, 0]\n",
    "data_11_y = data_manifold_11[:, 1]\n",
    "\n",
    "data_4_x = data_manifold_4[:, 0]\n",
    "data_4_y = data_manifold_4[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data_11_x, data_11_y, color=\"r\")\n",
    "plt.scatter(data_4_x, data_4_y, color=\"b\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
